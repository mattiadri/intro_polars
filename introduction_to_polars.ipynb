{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intoduction to Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://realpython.com/python-gil/\n",
    "\n",
    "https://pola.rs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/polars.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The versatility of Python and its simple syntax are certainly the strong points of this high-level, general-purpose programming language. However, one of its greatest weaknesses, if not the greatest (especially considering how the computing ecosystem has evolved), is the Global Interpreter Lock (GIL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python Global Interpreter Lock, or GIL, is a mutex (lock) that permits only one thread to control the Python interpreter at a time. In simple terms, it means that only one thread can be actively executing code at any given moment. While this constraint may not be noticeable in single-threaded programs, it can become a performance bottleneck in scenarios involving multi-threaded code or CPU-bound tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GIL's reputation as an \"infamous\" feature stems from its restriction of executing only one thread at a time, even in multi-threaded architectures with multiple CPU cores. This article explores how the GIL affects the performance of Python programs and provides insights into mitigating its impact on code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the problem the GIL addresses, it's crucial to delve into Python's memory management using reference counting. Objects in Python have a reference count variable that tracks the number of references pointing to the object. When this count reaches zero, the object's memory is released. The GIL addresses the challenge of protecting this reference count variable from race conditions where two threads might simultaneously increase or decrease its value. Without proper protection, such scenarios could lead to memory leaks or, worse, incorrect release of memory while references to the object still exist, resulting in crashes or unpredictable bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "a = []\n",
    "b = a\n",
    "sys.getrefcount(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While one solution could be adding locks to shared data structures to safeguard the reference count variable, it introduces the risk of deadlocks and performance degradation due to repeated lock acquisition and release. The GIL takes a different approach by acting as a single lock on the interpreter itself. This ensures that executing any Python bytecode necessitates acquiring the interpreter lock. Although this approach avoids deadlocks and minimizes performance overhead, it effectively confines CPU-bound Python programs to single-threaded execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is a high-level programming language that prioritizes ease of use and readability. However, this focus on simplicity and readability can impact its performance. Recognizing the need for optimized execution in certain scenarios, an interface between Python and C has been developed. This integration allows developers to leverage the efficiency of C, a low-level language known for its speed, in performance-critical sections of their Python programs. By doing so, they can strike a balance between Python's simplicity and C's performance, optimizing their applications for specific tasks. Interestingly, the GIL was not only designed to ensure better performance in single-threaded programs but also to facilitate the integration of C libraries that were not thread-safe. It's amusing to note that in C, you can sidestep the issue of individual threads, and this playful workaround extends to other programming languages as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those clever creators of Polars wrote the library in Rust. In addition to this, they focused on parallelization and efficiency. And given the excellent result, Polars will probably replace Pandas in the course of a few years. Therefore, in this course, we explore a bit of this fantastic library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import polars as pl\n",
    "except ImportError:\n",
    "    print(\"Il pacchetto 'polars' non è installato. Installazione in corso...\")\n",
    "    %conda install -c conda-forge polars -y\n",
    "    print(\"Installazione completata.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polars is entirely based on Arrow data types and backed by Arrow memory arrays. From this point of view, there isn't much new, but it's worth listing the types we will use the most:\n",
    "\n",
    "* ```pl.Int32``` and ```pl.Int64```\n",
    "* ```pl.Float32``` and ```pl.Float32```\n",
    "* ```pl.Date``` and ```pl.Datetime```\n",
    "* ```pl.Boolean``` and ```pl.Categorical```\n",
    "\n",
    "Categorical data represents string data where the values in the column have a finite set of values. Storing these values as plain strings is a waste of memory so Polars encode them in dictionary format.\n",
    "\n",
    "Keep in mind that in Polars, 'NaN' doesn't exist; instead, it is replaced with ```pl.Null```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the structures, they are similar to those in Pandas, so essentially, we will be working with:\n",
    "* ```pl.Series``` and  ```pl.DataFrame```\n",
    "\n",
    "Some of these functions have been implemented and operate for ```pl.DataFrame``` in the same way as those in Pandas:\n",
    "\n",
    "* ```.head()``` shows the first 5 elements\n",
    "* ```.tail()``` shows the last 5 elements\n",
    "* ```.sample()``` shows 5 random elements\n",
    "* ```.describe()``` returns summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polars supports reading and writing to all common files (e.g. csv, json, parquet), cloud storage (S3, Azure Blob, BigQuery) and databases (e.g. postgres, mysql). For this course, we are revisiting the old and dear Titanic database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the syntax is quite simple: use ```read_filetype``` for reading and ```write_filetype``` for writing. I would refer you to the documentation for the attributes of the aforementioned functions. A non-exhaustive list of file types includes:\n",
    "* ```.read_json()``` and ```.write_json()```\n",
    "* ```.read_parquet()``` and ```.write_parquet()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contexts and Expressions constitute the language through which Polars performs operations on data. A context pertains to the circumstances under which an expression is meant to be assessed. Let's do some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ```select``` context the selection applies expressions over columns. The expressions in this context must produce Series that are all the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌───────────────────────────────────┬──────┬──────────┐\n",
      "│ Name                              ┆ Age  ┆ Survived │\n",
      "│ ---                               ┆ ---  ┆ ---      │\n",
      "│ str                               ┆ f64  ┆ i64      │\n",
      "╞═══════════════════════════════════╪══════╪══════════╡\n",
      "│ Braund, Mr. Owen Harris           ┆ 22.0 ┆ 0        │\n",
      "│ Cumings, Mrs. John Bradley (Flor… ┆ 38.0 ┆ 1        │\n",
      "│ Heikkinen, Miss. Laina            ┆ 26.0 ┆ 1        │\n",
      "│ Futrelle, Mrs. Jacques Heath (Li… ┆ 35.0 ┆ 1        │\n",
      "│ Allen, Mr. William Henry          ┆ 35.0 ┆ 0        │\n",
      "└───────────────────────────────────┴──────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.select(pl.col(\"Name\"),\n",
    "                pl.col(\"Age\"),\n",
    "                pl.col(\"Survived\")\n",
    "                ).limit(5)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```select``` is very similar to ```with_columns``` The main difference is that the latter retains the original columns and adds new ones while ```select``` drops the original columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 4)\n",
      "┌───────────────────────────────────┬──────┬──────────┬─────────┐\n",
      "│ Name                              ┆ Age  ┆ Survived ┆ As bool │\n",
      "│ ---                               ┆ ---  ┆ ---      ┆ ---     │\n",
      "│ str                               ┆ f64  ┆ i64      ┆ bool    │\n",
      "╞═══════════════════════════════════╪══════╪══════════╪═════════╡\n",
      "│ Braund, Mr. Owen Harris           ┆ 22.0 ┆ 0        ┆ false   │\n",
      "│ Cumings, Mrs. John Bradley (Flor… ┆ 38.0 ┆ 1        ┆ true    │\n",
      "│ Heikkinen, Miss. Laina            ┆ 26.0 ┆ 1        ┆ true    │\n",
      "│ Futrelle, Mrs. Jacques Heath (Li… ┆ 35.0 ┆ 1        ┆ true    │\n",
      "│ Allen, Mr. William Henry          ┆ 35.0 ┆ 0        ┆ false   │\n",
      "└───────────────────────────────────┴──────┴──────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "out = out.with_columns(pl.col(\"Survived\").cast(pl.Boolean).alias(\"As bool\"))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the filter context you filter the existing dataframe based on arbitrary expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 12)\n",
      "┌─────────────┬──────────┬────────┬──────────────────┬───┬─────────┬─────────┬───────┬──────────┐\n",
      "│ PassengerId ┆ Survived ┆ Pclass ┆ Name             ┆ … ┆ Ticket  ┆ Fare    ┆ Cabin ┆ Embarked │\n",
      "│ ---         ┆ ---      ┆ ---    ┆ ---              ┆   ┆ ---     ┆ ---     ┆ ---   ┆ ---      │\n",
      "│ i64         ┆ i64      ┆ i64    ┆ str              ┆   ┆ str     ┆ f64     ┆ str   ┆ str      │\n",
      "╞═════════════╪══════════╪════════╪══════════════════╪═══╪═════════╪═════════╪═══════╪══════════╡\n",
      "│ 8           ┆ 0        ┆ 3      ┆ Palsson, Master. ┆ … ┆ 349909  ┆ 21.075  ┆ null  ┆ S        │\n",
      "│             ┆          ┆        ┆ Gosta Leonard    ┆   ┆         ┆         ┆       ┆          │\n",
      "│ 11          ┆ 1        ┆ 3      ┆ Sandstrom, Miss. ┆ … ┆ PP 9549 ┆ 16.7    ┆ G6    ┆ S        │\n",
      "│             ┆          ┆        ┆ Marguerite Rut   ┆   ┆         ┆         ┆       ┆          │\n",
      "│ 17          ┆ 0        ┆ 3      ┆ Rice, Master.    ┆ … ┆ 382652  ┆ 29.125  ┆ null  ┆ Q        │\n",
      "│             ┆          ┆        ┆ Eugene           ┆   ┆         ┆         ┆       ┆          │\n",
      "│ 25          ┆ 0        ┆ 3      ┆ Palsson, Miss.   ┆ … ┆ 349909  ┆ 21.075  ┆ null  ┆ S        │\n",
      "│             ┆          ┆        ┆ Torborg Danira   ┆   ┆         ┆         ┆       ┆          │\n",
      "│ 51          ┆ 0        ┆ 3      ┆ Panula, Master.  ┆ … ┆ 3101295 ┆ 39.6875 ┆ null  ┆ S        │\n",
      "│             ┆          ┆        ┆ Juha Niilo       ┆   ┆         ┆         ┆       ┆          │\n",
      "└─────────────┴──────────┴────────┴──────────────────┴───┴─────────┴─────────┴───────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.filter((pl.col(\"Age\") < 10) & (pl.col(\"Pclass\") == 3)).limit(5)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually the ```group_by``` context create a new DataFrame that includes the different 'groups' we want to group together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<polars.dataframe.group_by.GroupBy at 0x20d2e227610>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.group_by('Sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this gives an expression, in order to obtain a Dataframe or a Series we have to evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 2)\n",
      "┌────────┬───────┐\n",
      "│ Sex    ┆ count │\n",
      "│ ---    ┆ ---   │\n",
      "│ str    ┆ u32   │\n",
      "╞════════╪═══════╡\n",
      "│ male   ┆ 577   │\n",
      "│ female ┆ 314   │\n",
      "└────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.group_by('Sex', maintain_order=True).count()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ```group_by``` context, expressions work on groups and thus can yield results of any length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 4)\n",
      "┌────────┬─────┬─────────────────┬──────────┐\n",
      "│ Pclass ┆ #   ┆ ids of Johns    ┆ Survived │\n",
      "│ ---    ┆ --- ┆ ---             ┆ ---      │\n",
      "│ i64    ┆ u32 ┆ list[i64]       ┆ i64      │\n",
      "╞════════╪═════╪═════════════════╪══════════╡\n",
      "│ 1      ┆ 216 ┆ [2, 169, … 823] ┆ 136      │\n",
      "│ 2      ┆ 184 ┆ [42, 99, … 865] ┆ 87       │\n",
      "│ 3      ┆ 491 ┆ [9, 46, … 889]  ┆ 119      │\n",
      "└────────┴─────┴─────────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.group_by(\"Pclass\").agg(\n",
    "    pl.count(\"PassengerId\").alias(\"#\"),\n",
    "    pl.col(\"PassengerId\").filter(pl.col(\"Name\").str.contains(\"John\")).alias(\"ids of Johns\"),\n",
    "    pl.sum(\"Survived\")\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to be able to sort our DataFrame not only to view it but also to do operations as in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 3)\n",
      "┌────────┬────────────────────────────┬─────────────────────────────────┐\n",
      "│ Pclass ┆ richer                     ┆ poorer                          │\n",
      "│ ---    ┆ ---                        ┆ ---                             │\n",
      "│ i64    ┆ str                        ┆ str                             │\n",
      "╞════════╪════════════════════════════╪═════════════════════════════════╡\n",
      "│ 1      ┆ Ward, Miss. Anna           ┆ Reuchlin, Jonkheer. John George │\n",
      "│ 3      ┆ Sage, Master. Thomas Henry ┆ Johnson, Mr. Alfred             │\n",
      "│ 2      ┆ Hood, Mr. Ambrose Jr       ┆ Knight, Mr. Robert J            │\n",
      "└────────┴────────────────────────────┴─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "out = (df.sort(\"Fare\", descending=True).group_by(\"Pclass\").agg(\n",
    "        pl.col('Name').first().alias(\"richer\"),\n",
    "        pl.col('Name').last().alias(\"poorer\")).limit(3))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have so many expressions in polar that for the sake of brevity I will list only the most common ones, and I encourage you to consult the library guide for the complete list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical and Logical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the four operations and the logical relationships are supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 2)\n",
      "┌─────────┬──────────────────────┐\n",
      "│ ¢       ┆ Is a child survivor? │\n",
      "│ ---     ┆ ---                  │\n",
      "│ f64     ┆ bool                 │\n",
      "╞═════════╪══════════════════════╡\n",
      "│ 725.0   ┆ false                │\n",
      "│ 7128.33 ┆ false                │\n",
      "│ 792.5   ┆ false                │\n",
      "│ 5310.0  ┆ false                │\n",
      "│ 805.0   ┆ false                │\n",
      "└─────────┴──────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.select(\n",
    "        (pl.col('Fare') * 100).alias('¢'),\n",
    "        ((pl.col(\"Age\") <= 10) & (pl.col(\"Survived\") == 1.)).alias(\"Is a child survivor?\")).limit(5)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expression expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance this section might seem almost superfluous or otherwise simple and unimportant, but mastering \"Expression expansion\" can significantly reduce the code you will have to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 12)\n",
      "┌─────────────┬──────────┬────────┬───────────────────┬───┬───────────┬─────────┬───────┬──────────┐\n",
      "│ PassengerId ┆ Survived ┆ Pclass ┆ Name              ┆ … ┆ Ticket    ┆ Fare    ┆ Cabin ┆ Embarked │\n",
      "│ ---         ┆ ---      ┆ ---    ┆ ---               ┆   ┆ ---       ┆ ---     ┆ ---   ┆ ---      │\n",
      "│ i64         ┆ i64      ┆ i64    ┆ str               ┆   ┆ str       ┆ f64     ┆ str   ┆ str      │\n",
      "╞═════════════╪══════════╪════════╪═══════════════════╪═══╪═══════════╪═════════╪═══════╪══════════╡\n",
      "│ 1           ┆ 0        ┆ 3      ┆ Braund, Mr. Owen  ┆ … ┆ A/5 21171 ┆ 7.25    ┆ null  ┆ S        │\n",
      "│             ┆          ┆        ┆ Harris            ┆   ┆           ┆         ┆       ┆          │\n",
      "│ 2           ┆ 1        ┆ 1      ┆ Cumings, Mrs.     ┆ … ┆ PC 17599  ┆ 71.2833 ┆ C85   ┆ C        │\n",
      "│             ┆          ┆        ┆ John Bradley      ┆   ┆           ┆         ┆       ┆          │\n",
      "│             ┆          ┆        ┆ (Flor…            ┆   ┆           ┆         ┆       ┆          │\n",
      "│ 3           ┆ 1        ┆ 3      ┆ Heikkinen, Miss.  ┆ … ┆ STON/O2.  ┆ 7.925   ┆ null  ┆ S        │\n",
      "│             ┆          ┆        ┆ Laina             ┆   ┆ 3101282   ┆         ┆       ┆          │\n",
      "└─────────────┴──────────┴────────┴───────────────────┴───┴───────────┴─────────┴───────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.select(pl.col(\"*\")).limit(3) # equivalent to df.select(pl.all())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can select one or more columns by simply typing its name in the expression ```pl.col```, or we can exclude from visualization with:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 7)\n",
      "┌─────────────┬──────────┬────────┬────────┬──────┬───────┬─────────┐\n",
      "│ PassengerId ┆ Survived ┆ Pclass ┆ Sex    ┆ Age  ┆ SibSp ┆ Fare    │\n",
      "│ ---         ┆ ---      ┆ ---    ┆ ---    ┆ ---  ┆ ---   ┆ ---     │\n",
      "│ i64         ┆ i64      ┆ i64    ┆ str    ┆ f64  ┆ i64   ┆ f64     │\n",
      "╞═════════════╪══════════╪════════╪════════╪══════╪═══════╪═════════╡\n",
      "│ 1           ┆ 0        ┆ 3      ┆ male   ┆ 22.0 ┆ 1     ┆ 7.25    │\n",
      "│ 2           ┆ 1        ┆ 1      ┆ female ┆ 38.0 ┆ 1     ┆ 71.2833 │\n",
      "│ 3           ┆ 1        ┆ 3      ┆ female ┆ 26.0 ┆ 0     ┆ 7.925   │\n",
      "└─────────────┴──────────┴────────┴────────┴──────┴───────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.select(pl.all().exclude(\"Ticket\",\"Embarked\",\"Name\",\"Cabin\",\"Parch\")).limit(3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also select columns by type, regular expressions, or using selectors, which I invite you to view on your own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column naming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an operation on a column does not change its name if we want to do it we have to use the ```.alias()``` function as we have partly already seen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 2)\n",
      "┌─────────┬───────────┐\n",
      "│ ¢       ┆ k$        │\n",
      "│ ---     ┆ ---       │\n",
      "│ f64     ┆ f64       │\n",
      "╞═════════╪═══════════╡\n",
      "│ 725.0   ┆ 0.00725   │\n",
      "│ 7128.33 ┆ 0.0712833 │\n",
      "│ 792.5   ┆ 0.007925  │\n",
      "└─────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.select((pl.col('Fare') * 100).alias('¢'),\n",
    "          (pl.col('Fare') / 1000).alias('k$')).limit(3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique values can be counted with two algorithms, one exact and one approximated with HyperLogLog++. The advantage of approximation occurs when we have many rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 2)\n",
      "┌────────┬───────────────┐\n",
      "│ unique ┆ unique_approx │\n",
      "│ ---    ┆ ---           │\n",
      "│ u32    ┆ u32           │\n",
      "╞════════╪═══════════════╡\n",
      "│ 891    ┆ 891           │\n",
      "└────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.select(\n",
    "    pl.col(\"Name\").n_unique().alias(\"unique\"),\n",
    "    pl.approx_n_unique(\"Name\").alias(\"unique_approx\"))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conditionals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polars supports if-else like conditions in expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 2)\n",
      "┌──────┬─────────────┐\n",
      "│ Age  ┆ conditional │\n",
      "│ ---  ┆ ---         │\n",
      "│ f64  ┆ bool        │\n",
      "╞══════╪═════════════╡\n",
      "│ 22.0 ┆ true        │\n",
      "│ 38.0 ┆ true        │\n",
      "│ 26.0 ┆ true        │\n",
      "│ 35.0 ┆ true        │\n",
      "│ 35.0 ┆ true        │\n",
      "└──────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.select(\n",
    "    pl.col(\"Age\"),\n",
    "    pl.when(pl.col(\"Age\") > 18)\n",
    "    .then(pl.lit(True))\n",
    "    .otherwise(pl.lit(False))\n",
    "    .alias(\"conditional\")).limit(5)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicate is placed in the when clause, and if its evaluation is true, the then expression is applied, otherwise the otherwise expression is applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Casting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polars uses Arrow to manage the data in memory and relies on Rust to do the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>PassengerId</th><th>Survived</th><th>Pclass</th><th>Name</th><th>Sex</th><th>Age</th><th>SibSp</th><th>Parch</th><th>Ticket</th><th>Fare</th><th>Cabin</th><th>Embarked</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>f64</td><td>i64</td><td>i64</td><td>str</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>0</td><td>3</td><td>&quot;Braund, Mr. Ow…</td><td>&quot;male&quot;</td><td>22.0</td><td>1</td><td>0</td><td>&quot;A/5 21171&quot;</td><td>7.25</td><td>null</td><td>&quot;S&quot;</td></tr><tr><td>2</td><td>1</td><td>1</td><td>&quot;Cumings, Mrs. …</td><td>&quot;female&quot;</td><td>38.0</td><td>1</td><td>0</td><td>&quot;PC 17599&quot;</td><td>71.2833</td><td>&quot;C85&quot;</td><td>&quot;C&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 12)\n",
       "┌─────────────┬──────────┬────────┬───────────────────┬───┬───────────┬─────────┬───────┬──────────┐\n",
       "│ PassengerId ┆ Survived ┆ Pclass ┆ Name              ┆ … ┆ Ticket    ┆ Fare    ┆ Cabin ┆ Embarked │\n",
       "│ ---         ┆ ---      ┆ ---    ┆ ---               ┆   ┆ ---       ┆ ---     ┆ ---   ┆ ---      │\n",
       "│ i64         ┆ i64      ┆ i64    ┆ str               ┆   ┆ str       ┆ f64     ┆ str   ┆ str      │\n",
       "╞═════════════╪══════════╪════════╪═══════════════════╪═══╪═══════════╪═════════╪═══════╪══════════╡\n",
       "│ 1           ┆ 0        ┆ 3      ┆ Braund, Mr. Owen  ┆ … ┆ A/5 21171 ┆ 7.25    ┆ null  ┆ S        │\n",
       "│             ┆          ┆        ┆ Harris            ┆   ┆           ┆         ┆       ┆          │\n",
       "│ 2           ┆ 1        ┆ 1      ┆ Cumings, Mrs.     ┆ … ┆ PC 17599  ┆ 71.2833 ┆ C85   ┆ C        │\n",
       "│             ┆          ┆        ┆ John Bradley      ┆   ┆           ┆         ┆       ┆          │\n",
       "│             ┆          ┆        ┆ (Flor…            ┆   ┆           ┆         ┆       ┆          │\n",
       "└─────────────┴──────────┴────────┴───────────────────┴───┴───────────┴─────────┴───────┴──────────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 3)\n",
      "┌──────┬────────┬──────────┐\n",
      "│ Fare ┆ Pclass ┆ Survived │\n",
      "│ ---  ┆ ---    ┆ ---      │\n",
      "│ i32  ┆ f32    ┆ bool     │\n",
      "╞══════╪════════╪══════════╡\n",
      "│ 7    ┆ 3.0    ┆ false    │\n",
      "│ 71   ┆ 1.0    ┆ true     │\n",
      "│ 7    ┆ 3.0    ┆ true     │\n",
      "└──────┴────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.select(pl.col('Fare').cast(pl.Int32), #Note that in the case of decimal values these are rounded downwards when casting to an integer.\n",
    "                pl.col('Pclass').cast(pl.String).cast(pl.Float32),\n",
    "                pl.col('Survived').cast(pl.Boolean)).limit(3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cast method includes a strict parameter that determines the behavior of Polars when it encounters a value that cannot be converted from the source DataType to the target DataType. By default, strict=True means that Polars will throw an error to notify the user of the failed conversion and provide details about the values that could not be cast. If strict=False instead, values that cannot be converted to the target DataType will be safely converted to null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to convert dates to string and viceversa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 2)\n",
      "┌────────────┬─────────────────────┐\n",
      "│ date       ┆ string              │\n",
      "│ ---        ┆ ---                 │\n",
      "│ str        ┆ datetime[μs]        │\n",
      "╞════════════╪═════════════════════╡\n",
      "│ 2020-02-19 ┆ 2020-02-19 00:00:00 │\n",
      "│ 2021-02-19 ┆ 2021-02-19 00:00:00 │\n",
      "│ 2022-02-19 ┆ 2022-02-19 00:00:00 │\n",
      "│ 2023-02-19 ┆ 2023-02-19 00:00:00 │\n",
      "│ 2024-02-19 ┆ 2024-02-23 00:00:00 │\n",
      "└────────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime\n",
    "\n",
    "df_dates = pl.DataFrame(\n",
    "    {\n",
    "        \"date\": pl.date_range(date(2020, 2, 19), date(2024, 2, 23), interval='1y', eager=True),\n",
    "        \"string\": [\"2020-02-19\",\"2021-02-19\",\"2022-02-19\",\"2023-02-19\",\"2024-02-23\",],\n",
    "        \"values\": [12,3,4,5,7],\n",
    "        \"groups\": ['a','b','b','c','a']\n",
    "    }\n",
    ")\n",
    "\n",
    "out = df_dates.select(\n",
    "    pl.col(\"date\").dt.to_string(\"%Y-%m-%d\"),\n",
    "    pl.col(\"string\").str.to_datetime(\"%Y-%m-%d\"),\n",
    ")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String processing can often be inefficient because of their unpredictable memory size, which forces the CPU to access many random memory locations. To solve this problem, Polars uses Arrow as a backend, which stores all strings in a contiguous block of memory. The string namespace can be accessed through the ```.str``` attribute and then we can perform our operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 5)\n",
      "┌───────────────────────────────────┬───────┬─────────┬─────────────┬───────────┐\n",
      "│ Name                              ┆ regex ┆ literal ┆ starts_with ┆ ends_with │\n",
      "│ ---                               ┆ ---   ┆ ---     ┆ ---         ┆ ---       │\n",
      "│ str                               ┆ bool  ┆ bool    ┆ bool        ┆ bool      │\n",
      "╞═══════════════════════════════════╪═══════╪═════════╪═════════════╪═══════════╡\n",
      "│ Braund, Mr. Owen Harris           ┆ true  ┆ false   ┆ false       ┆ false     │\n",
      "│ Cumings, Mrs. John Bradley (Flor… ┆ true  ┆ false   ┆ false       ┆ false     │\n",
      "│ Heikkinen, Miss. Laina            ┆ false ┆ false   ┆ false       ┆ false     │\n",
      "└───────────────────────────────────┴───────┴─────────┴─────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.select(\n",
    "    pl.col(\"Name\"),\n",
    "    pl.col(\"Name\").str.contains(\"Mr|Mrs\").alias(\"regex\"),\n",
    "    pl.col(\"Name\").str.contains(\"Miss$\", literal=True).alias(\"literal\"),\n",
    "    pl.col(\"Name\").str.starts_with(\"A\").alias(\"starts_with\"),\n",
    "    pl.col(\"Name\").str.ends_with(\"Z\").alias(\"ends_with\")).limit(3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also extract a pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 1)\n",
      "┌───────┐\n",
      "│ Name  │\n",
      "│ ---   │\n",
      "│ str   │\n",
      "╞═══════╡\n",
      "│ Owen  │\n",
      "│ John  │\n",
      "│ Laina │\n",
      "└───────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.select(pl.col(\"Name\").str.extract(r\"\\.\\s*(\\w+)\", group_index=1)).limit(3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this will provide only the 'group_index' occurrence, if we want all occurrences we have to use ```str.extract_all()```.\n",
    "We can even do some replace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 1)\n",
      "┌───────────────────────────────────┐\n",
      "│ Name                              │\n",
      "│ ---                               │\n",
      "│ str                               │\n",
      "╞═══════════════════════════════════╡\n",
      "│ Braund, Mx. Owen Harris           │\n",
      "│ Cumings, Mx. John Bradley (Flore… │\n",
      "│ Heikkinen, Mx. Laina              │\n",
      "└───────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.select(pl.col(\"Name\").str.replace(r\"(Mrs|Miss|Mr)\", \"Mx\")).limit(3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data are represented in Arrow and Polars with a ```pl.Null``` value. This null missing value applies to all data types, including numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 12)\n",
      "┌─────────────┬──────────┬────────┬──────┬───┬────────┬──────┬───────┬──────────┐\n",
      "│ PassengerId ┆ Survived ┆ Pclass ┆ Name ┆ … ┆ Ticket ┆ Fare ┆ Cabin ┆ Embarked │\n",
      "│ ---         ┆ ---      ┆ ---    ┆ ---  ┆   ┆ ---    ┆ ---  ┆ ---   ┆ ---      │\n",
      "│ u32         ┆ u32      ┆ u32    ┆ u32  ┆   ┆ u32    ┆ u32  ┆ u32   ┆ u32      │\n",
      "╞═════════════╪══════════╪════════╪══════╪═══╪════════╪══════╪═══════╪══════════╡\n",
      "│ 0           ┆ 0        ┆ 0      ┆ 0    ┆ … ┆ 0      ┆ 0    ┆ 687   ┆ 2        │\n",
      "└─────────────┴──────────┴────────┴──────┴───┴────────┴──────┴───────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.null_count()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replace null values we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 12)\n",
      "┌─────────────┬──────────┬────────┬──────────────┬───┬─────────────┬─────────┬──────────┬──────────┐\n",
      "│ PassengerId ┆ Survived ┆ Pclass ┆ Name         ┆ … ┆ Ticket      ┆ Fare    ┆ Cabin    ┆ Embarked │\n",
      "│ ---         ┆ ---      ┆ ---    ┆ ---          ┆   ┆ ---         ┆ ---     ┆ ---      ┆ ---      │\n",
      "│ i64         ┆ i64      ┆ i64    ┆ str          ┆   ┆ str         ┆ f64     ┆ str      ┆ str      │\n",
      "╞═════════════╪══════════╪════════╪══════════════╪═══╪═════════════╪═════════╪══════════╪══════════╡\n",
      "│ 1           ┆ 0        ┆ 3      ┆ Braund, Mr.  ┆ … ┆ A/5 21171   ┆ 7.25    ┆ No cabin ┆ S        │\n",
      "│             ┆          ┆        ┆ Owen Harris  ┆   ┆             ┆         ┆          ┆          │\n",
      "│ 2           ┆ 1        ┆ 1      ┆ Cumings,     ┆ … ┆ PC 17599    ┆ 71.2833 ┆ C85      ┆ C        │\n",
      "│             ┆          ┆        ┆ Mrs. John    ┆   ┆             ┆         ┆          ┆          │\n",
      "│             ┆          ┆        ┆ Bradley      ┆   ┆             ┆         ┆          ┆          │\n",
      "│             ┆          ┆        ┆ (Flor…       ┆   ┆             ┆         ┆          ┆          │\n",
      "│ 3           ┆ 1        ┆ 3      ┆ Heikkinen,   ┆ … ┆ STON/O2.    ┆ 7.925   ┆ No cabin ┆ S        │\n",
      "│             ┆          ┆        ┆ Miss. Laina  ┆   ┆ 3101282     ┆         ┆          ┆          │\n",
      "└─────────────┴──────────┴────────┴──────────────┴───┴─────────────┴─────────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.with_columns(pl.col(\"Cabin\").fill_null(pl.lit('No cabin'))).limit(3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we use the ```fill_null``` function we can also establish a strategy as in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The over function allows operations per group useful for example in creating new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 13)\n",
      "┌─────────────┬──────────┬────────┬───────────────┬───┬─────────┬───────┬──────────┬───────────────┐\n",
      "│ PassengerId ┆ Survived ┆ Pclass ┆ Name          ┆ … ┆ Fare    ┆ Cabin ┆ Embarked ┆ avg Fare per  │\n",
      "│ ---         ┆ ---      ┆ ---    ┆ ---           ┆   ┆ ---     ┆ ---   ┆ ---      ┆ class         │\n",
      "│ i64         ┆ i64      ┆ i64    ┆ str           ┆   ┆ f64     ┆ str   ┆ str      ┆ ---           │\n",
      "│             ┆          ┆        ┆               ┆   ┆         ┆       ┆          ┆ f64           │\n",
      "╞═════════════╪══════════╪════════╪═══════════════╪═══╪═════════╪═══════╪══════════╪═══════════════╡\n",
      "│ 1           ┆ 0        ┆ 3      ┆ Braund, Mr.   ┆ … ┆ 7.25    ┆ null  ┆ S        ┆ 13.67555      │\n",
      "│             ┆          ┆        ┆ Owen Harris   ┆   ┆         ┆       ┆          ┆               │\n",
      "│ 2           ┆ 1        ┆ 1      ┆ Cumings, Mrs. ┆ … ┆ 71.2833 ┆ C85   ┆ C        ┆ 84.154687     │\n",
      "│             ┆          ┆        ┆ John Bradley  ┆   ┆         ┆       ┆          ┆               │\n",
      "│             ┆          ┆        ┆ (Flor…        ┆   ┆         ┆       ┆          ┆               │\n",
      "│ 3           ┆ 1        ┆ 3      ┆ Heikkinen,    ┆ … ┆ 7.925   ┆ null  ┆ S        ┆ 13.67555      │\n",
      "│             ┆          ┆        ┆ Miss. Laina   ┆   ┆         ┆       ┆          ┆               │\n",
      "└─────────────┴──────────┴────────┴───────────────┴───┴─────────┴───────┴──────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.with_columns(pl.col(\"Fare\").mean().over(\"Pclass\").alias('avg Fare per class')).limit(3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's verify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 2)\n",
      "┌────────┬───────────┐\n",
      "│ Pclass ┆ Fare      │\n",
      "│ ---    ┆ ---       │\n",
      "│ i64    ┆ f64       │\n",
      "╞════════╪═══════════╡\n",
      "│ 3      ┆ 13.67555  │\n",
      "│ 1      ┆ 84.154687 │\n",
      "│ 2      ┆ 20.662183 │\n",
      "└────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.select(pl.col('Fare','Pclass')).group_by('Pclass').mean()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folds are used for accumulating on multiple columns horizontally and can be very useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 1)\n",
      "┌──────────────┐\n",
      "│ count if > 1 │\n",
      "│ ---          │\n",
      "│ i32          │\n",
      "╞══════════════╡\n",
      "│ 1            │\n",
      "│ 2            │\n",
      "│ 1            │\n",
      "└──────────────┘\n"
     ]
    }
   ],
   "source": [
    "df_fold = pl.DataFrame({\"a\": [1, 2, 3],\"b\": [4, 5, 1]})\n",
    "\n",
    "out = df_fold.select(\n",
    "    pl.fold(acc=pl.lit(0), function=lambda acc, x: acc + x, exprs=pl.all()>1).alias(\"count if > 1\"))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are two ways to combine DataFrames in polars, methods that we also seen in pandas but first we create two dataframes to use then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "df1 = pl.DataFrame(\n",
    "    {\n",
    "        \"index\": range(8),\n",
    "        \"random numbers\": [random.random() for _ in range(8)],\n",
    "        \"x\": [1, 2.0, float(\"nan\"), float(\"nan\"), 0, -5, -42, None],\n",
    "    }\n",
    ")\n",
    "\n",
    "df2 = pl.DataFrame(\n",
    "    {\n",
    "        \"id\": range(8),\n",
    "        \"random letters\": [random.choice(string.ascii_letters) for _ in range(8)],\n",
    "    }\n",
    ")\n",
    "\n",
    "df3 = pl.DataFrame(\n",
    "    {\n",
    "        \"index\": 8,\n",
    "        \"random numbers\": random.random(),\n",
    "        \"x\": 20.0,\n",
    "        \n",
    "\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The join is a cotuct borrowed from the sql language different types of joins are implemented for their explanation I refer to the literature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (8, 4)\n",
      "┌───────┬────────────────┬───────┬────────────────┐\n",
      "│ index ┆ random numbers ┆ x     ┆ random letters │\n",
      "│ ---   ┆ ---            ┆ ---   ┆ ---            │\n",
      "│ i64   ┆ f64            ┆ f64   ┆ str            │\n",
      "╞═══════╪════════════════╪═══════╪════════════════╡\n",
      "│ 0     ┆ 0.066844       ┆ 1.0   ┆ O              │\n",
      "│ 1     ┆ 0.589281       ┆ 2.0   ┆ b              │\n",
      "│ 2     ┆ 0.417543       ┆ NaN   ┆ V              │\n",
      "│ 3     ┆ 0.164582       ┆ NaN   ┆ e              │\n",
      "│ 4     ┆ 0.814405       ┆ 0.0   ┆ C              │\n",
      "│ 5     ┆ 0.676169       ┆ -5.0  ┆ E              │\n",
      "│ 6     ┆ 0.644668       ┆ -42.0 ┆ e              │\n",
      "│ 7     ┆ 0.894817       ┆ null  ┆ a              │\n",
      "└───────┴────────────────┴───────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "joined = df1.join(df2, left_on=\"index\", right_on=\"id\")\n",
    "print(joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat is short for concatenate; you can concatenate both vertically and horizontally. Of course, one must be careful about size when using this construct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (8, 5)\n",
      "┌───────┬────────────────┬───────┬─────┬────────────────┐\n",
      "│ index ┆ random numbers ┆ x     ┆ id  ┆ random letters │\n",
      "│ ---   ┆ ---            ┆ ---   ┆ --- ┆ ---            │\n",
      "│ i64   ┆ f64            ┆ f64   ┆ i64 ┆ str            │\n",
      "╞═══════╪════════════════╪═══════╪═════╪════════════════╡\n",
      "│ 0     ┆ 0.066844       ┆ 1.0   ┆ 0   ┆ O              │\n",
      "│ 1     ┆ 0.589281       ┆ 2.0   ┆ 1   ┆ b              │\n",
      "│ 2     ┆ 0.417543       ┆ NaN   ┆ 2   ┆ V              │\n",
      "│ 3     ┆ 0.164582       ┆ NaN   ┆ 3   ┆ e              │\n",
      "│ 4     ┆ 0.814405       ┆ 0.0   ┆ 4   ┆ C              │\n",
      "│ 5     ┆ 0.676169       ┆ -5.0  ┆ 5   ┆ E              │\n",
      "│ 6     ┆ 0.644668       ┆ -42.0 ┆ 6   ┆ e              │\n",
      "│ 7     ┆ 0.894817       ┆ null  ┆ 7   ┆ a              │\n",
      "└───────┴────────────────┴───────┴─────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "hstacked = df1.hstack(df2)\n",
    "print(hstacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when using hstack also pay attention to column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (9, 3)\n",
      "┌───────┬────────────────┬───────┐\n",
      "│ index ┆ random numbers ┆ x     │\n",
      "│ ---   ┆ ---            ┆ ---   │\n",
      "│ i64   ┆ f64            ┆ f64   │\n",
      "╞═══════╪════════════════╪═══════╡\n",
      "│ 0     ┆ 0.066844       ┆ 1.0   │\n",
      "│ 1     ┆ 0.589281       ┆ 2.0   │\n",
      "│ 2     ┆ 0.417543       ┆ NaN   │\n",
      "│ 3     ┆ 0.164582       ┆ NaN   │\n",
      "│ 4     ┆ 0.814405       ┆ 0.0   │\n",
      "│ 5     ┆ 0.676169       ┆ -5.0  │\n",
      "│ 6     ┆ 0.644668       ┆ -42.0 │\n",
      "│ 7     ┆ 0.894817       ┆ null  │\n",
      "│ 8     ┆ 0.913987       ┆ 20.0  │\n",
      "└───────┴────────────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "vstacked = df1.vstack(df3)\n",
    "print(vstacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pivots and Melts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivots are operations that transform a column of a DataFrame into a new dimension by aggregating values according to a chosen function. In contrast, melts are the inverse operations that deconstruct a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1_782, 3)\n",
      "┌─────────────┬──────────┬─────────┐\n",
      "│ PassengerId ┆ variable ┆ value   │\n",
      "│ ---         ┆ ---      ┆ ---     │\n",
      "│ i64         ┆ str      ┆ f64     │\n",
      "╞═════════════╪══════════╪═════════╡\n",
      "│ 1           ┆ Fare     ┆ 7.25    │\n",
      "│ 2           ┆ Fare     ┆ 71.2833 │\n",
      "│ 3           ┆ Fare     ┆ 7.925   │\n",
      "│ 4           ┆ Fare     ┆ 53.1    │\n",
      "│ …           ┆ …        ┆ …       │\n",
      "│ 888         ┆ Survived ┆ 1.0     │\n",
      "│ 889         ┆ Survived ┆ 0.0     │\n",
      "│ 890         ┆ Survived ┆ 1.0     │\n",
      "│ 891         ┆ Survived ┆ 0.0     │\n",
      "└─────────────┴──────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df.melt(id_vars=['PassengerId'], value_vars=['Fare','Survived'])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (891, 3)\n",
      "┌─────────────┬─────────┬──────────┐\n",
      "│ PassengerId ┆ Fare    ┆ Survived │\n",
      "│ ---         ┆ ---     ┆ ---      │\n",
      "│ i64         ┆ f64     ┆ f64      │\n",
      "╞═════════════╪═════════╪══════════╡\n",
      "│ 1           ┆ 7.25    ┆ 0.0      │\n",
      "│ 2           ┆ 71.2833 ┆ 1.0      │\n",
      "│ 3           ┆ 7.925   ┆ 1.0      │\n",
      "│ 4           ┆ 53.1    ┆ 1.0      │\n",
      "│ …           ┆ …       ┆ …        │\n",
      "│ 888         ┆ 30.0    ┆ 1.0      │\n",
      "│ 889         ┆ 23.45   ┆ 0.0      │\n",
      "│ 890         ┆ 30.0    ┆ 1.0      │\n",
      "│ 891         ┆ 7.75    ┆ 0.0      │\n",
      "└─────────────┴─────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "out = out.pivot(index=['PassengerId'], columns='variable', values='value')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with time series, it is important to code dates in the correct manner because polars has native support for analyzing time series data to perform more sophisticated operations such as time grouping and resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to cast in the right format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 5)\n",
      "┌────────────┬────────────┬────────┬────────┬─────────────────────┐\n",
      "│ date       ┆ string     ┆ values ┆ groups ┆ datetime            │\n",
      "│ ---        ┆ ---        ┆ ---    ┆ ---    ┆ ---                 │\n",
      "│ date       ┆ str        ┆ i64    ┆ str    ┆ datetime[μs]        │\n",
      "╞════════════╪════════════╪════════╪════════╪═════════════════════╡\n",
      "│ 2020-02-19 ┆ 2020-02-19 ┆ 12     ┆ a      ┆ 2020-02-19 00:00:00 │\n",
      "│ 2021-02-19 ┆ 2021-02-19 ┆ 3      ┆ b      ┆ 2021-02-19 00:00:00 │\n",
      "│ 2022-02-19 ┆ 2022-02-19 ┆ 4      ┆ b      ┆ 2022-02-19 00:00:00 │\n",
      "│ 2023-02-19 ┆ 2023-02-19 ┆ 5      ┆ c      ┆ 2023-02-19 00:00:00 │\n",
      "│ 2024-02-19 ┆ 2024-02-23 ┆ 7      ┆ a      ┆ 2024-02-19 00:00:00 │\n",
      "└────────────┴────────────┴────────┴────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "df_dates = df_dates.with_columns(pl.col(\"date\").cast(pl.Datetime).alias('datetime'))\n",
    "print(df_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract date features from a date column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 6)\n",
      "┌────────────┬────────────┬────────┬────────┬─────────────────────┬──────┐\n",
      "│ date       ┆ string     ┆ values ┆ groups ┆ datetime            ┆ year │\n",
      "│ ---        ┆ ---        ┆ ---    ┆ ---    ┆ ---                 ┆ ---  │\n",
      "│ date       ┆ str        ┆ i64    ┆ str    ┆ datetime[μs]        ┆ i32  │\n",
      "╞════════════╪════════════╪════════╪════════╪═════════════════════╪══════╡\n",
      "│ 2020-02-19 ┆ 2020-02-19 ┆ 12     ┆ a      ┆ 2020-02-19 00:00:00 ┆ 2020 │\n",
      "│ 2021-02-19 ┆ 2021-02-19 ┆ 3      ┆ b      ┆ 2021-02-19 00:00:00 ┆ 2021 │\n",
      "│ 2022-02-19 ┆ 2022-02-19 ┆ 4      ┆ b      ┆ 2022-02-19 00:00:00 ┆ 2022 │\n",
      "│ 2023-02-19 ┆ 2023-02-19 ┆ 5      ┆ c      ┆ 2023-02-19 00:00:00 ┆ 2023 │\n",
      "│ 2024-02-19 ┆ 2024-02-23 ┆ 7      ┆ a      ┆ 2024-02-19 00:00:00 ┆ 2024 │\n",
      "└────────────┴────────────┴────────┴────────┴─────────────────────┴──────┘\n"
     ]
    }
   ],
   "source": [
    "out = df_dates.with_columns(pl.col(\"date\").dt.year().alias(\"year\"))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with time zones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 1)\n",
      "┌───────────────────────────────┐\n",
      "│ datetime                      │\n",
      "│ ---                           │\n",
      "│ datetime[μs, Europe/Brussels] │\n",
      "╞═══════════════════════════════╡\n",
      "│ 2020-02-19 01:00:00 CET       │\n",
      "│ 2021-02-19 01:00:00 CET       │\n",
      "│ 2022-02-19 01:00:00 CET       │\n",
      "│ 2023-02-19 01:00:00 CET       │\n",
      "│ 2024-02-19 01:00:00 CET       │\n",
      "└───────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df_dates.select(pl.col('datetime').dt.replace_time_zone(\"UTC\").dt.convert_time_zone(\"Europe/Brussels\"))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter in a better way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 5)\n",
      "┌────────────┬────────────┬────────┬────────┬─────────────────────┐\n",
      "│ date       ┆ string     ┆ values ┆ groups ┆ datetime            │\n",
      "│ ---        ┆ ---        ┆ ---    ┆ ---    ┆ ---                 │\n",
      "│ date       ┆ str        ┆ i64    ┆ str    ┆ datetime[μs]        │\n",
      "╞════════════╪════════════╪════════╪════════╪═════════════════════╡\n",
      "│ 2020-02-19 ┆ 2020-02-19 ┆ 12     ┆ a      ┆ 2020-02-19 00:00:00 │\n",
      "└────────────┴────────────┴────────┴────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df_dates.filter(pl.col(\"datetime\") == datetime(2020, 2, 19))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or even:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 5)\n",
      "┌────────────┬────────────┬────────┬────────┬─────────────────────┐\n",
      "│ date       ┆ string     ┆ values ┆ groups ┆ datetime            │\n",
      "│ ---        ┆ ---        ┆ ---    ┆ ---    ┆ ---                 │\n",
      "│ date       ┆ str        ┆ i64    ┆ str    ┆ datetime[μs]        │\n",
      "╞════════════╪════════════╪════════╪════════╪═════════════════════╡\n",
      "│ 2020-02-19 ┆ 2020-02-19 ┆ 12     ┆ a      ┆ 2020-02-19 00:00:00 │\n",
      "│ 2021-02-19 ┆ 2021-02-19 ┆ 3      ┆ b      ┆ 2021-02-19 00:00:00 │\n",
      "└────────────┴────────────┴────────┴────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df_dates.filter(pl.col(\"datetime\").is_between(datetime(2020, 1, 1), datetime(2022, 1, 1)))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even have a very nice function for grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (7, 3)\n",
      "┌────────┬─────────────────────┬────────┐\n",
      "│ groups ┆ datetime            ┆ values │\n",
      "│ ---    ┆ ---                 ┆ ---    │\n",
      "│ str    ┆ datetime[μs]        ┆ f64    │\n",
      "╞════════╪═════════════════════╪════════╡\n",
      "│ a      ┆ 2018-01-01 00:00:00 ┆ 12.0   │\n",
      "│ a      ┆ 2020-01-01 00:00:00 ┆ 12.0   │\n",
      "│ a      ┆ 2022-01-01 00:00:00 ┆ 7.0    │\n",
      "│ a      ┆ 2024-01-01 00:00:00 ┆ 7.0    │\n",
      "│ b      ┆ 2020-01-01 00:00:00 ┆ 3.5    │\n",
      "│ b      ┆ 2022-01-01 00:00:00 ┆ 4.0    │\n",
      "│ c      ┆ 2022-01-01 00:00:00 ┆ 5.0    │\n",
      "└────────┴─────────────────────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df_dates.group_by_dynamic(\"datetime\", every=\"2y\", period=\"3y\", by=\"groups\").agg(pl.col(\"values\").mean())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we can even downsample the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 5)\n",
      "┌─────────────────────┬────────────┬────────────┬────────┬────────┐\n",
      "│ datetime            ┆ date       ┆ string     ┆ values ┆ groups │\n",
      "│ ---                 ┆ ---        ┆ ---        ┆ ---    ┆ ---    │\n",
      "│ datetime[μs]        ┆ date       ┆ str        ┆ i64    ┆ str    │\n",
      "╞═════════════════════╪════════════╪════════════╪════════╪════════╡\n",
      "│ 2020-02-19 00:00:00 ┆ 2020-02-19 ┆ 2020-02-19 ┆ 12     ┆ a      │\n",
      "│ 2020-08-19 00:00:00 ┆ 2020-02-19 ┆ 2020-02-19 ┆ 12     ┆ a      │\n",
      "│ 2021-02-19 00:00:00 ┆ 2021-02-19 ┆ 2021-02-19 ┆ 3      ┆ b      │\n",
      "│ 2021-08-19 00:00:00 ┆ 2021-02-19 ┆ 2021-02-19 ┆ 3      ┆ b      │\n",
      "│ 2022-02-19 00:00:00 ┆ 2022-02-19 ┆ 2022-02-19 ┆ 4      ┆ b      │\n",
      "└─────────────────────┴────────────┴────────────┴────────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "out = df_dates.upsample(time_column=\"datetime\", every=\"6mo\").fill_null(strategy=\"forward\").limit(5)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about downsampling? Think of it as homework!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
